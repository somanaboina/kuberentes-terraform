resource "aws_eks_cluster" "cluster" {
  name = "${var.project_name}-eks-cluster"
  role_arn = var.eks_main_role_arn
  version = var.cluster_version
}

# where do you wanna place your network interface public or private

vpc_config {
  subnet_ids              = [var.pub_sub_1_a_id, var.pub_sub_2_b_cidr]
  endpoint_private_access = var.cluster_endpoint_private_access
  endpoint_public_access  = var.cluster_endpoint_public_access
  public_access_cidrs     = var.cluster_endpoint_access_cidr
}
#kubernetes_network_config {
#  service_ipv4_cidr  = var.cluster_svc_cidr
# }

#enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]
enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]

# Ensure that IAM Role permissions are created before and deleted after EKS Cluster handling.
  # Otherwise, EKS will not be able to properly delete EKS managed EC2 infrastructure such as Security Groups.
  # depends_on = [
  #   aws_iam_role_policy_attachment.example-AmazonEKSClusterPolicy,
  #   aws_iam_role_policy_attachment.example-AmazonEKSVPCResourceController,
  # ]
  tags = {
    Name = "${var.project_name}-eks-cluster"
    env = "${var.ENV}"
  }
}

# PUBLIC Nodegroup
/*
resource "aws_eks_node_group" "public_nodegroup" {
  cluster_name = aws_eks_cluster.cluster.name
  node_group_name = "${var.project_name}-eks-public-nodegroup"
  node_role_arn = var.EKS.eks_main_nodegroup_role_arn
  subnet_ids = [var.pub_sub_1_a_id, var.pub_sub_2_b_id]
  version = var.cluster_version
  ami_type = var.AMI_type
  instance_types = var.instance_type
  capacity_type = "on_demand"
  disk_size = 20
  scaling_config {
    desired_size = 2
    max_size = 4
    min_size = 2
  }
  update_config {
    max_unavailable = 1
  }
  remote_access {
    ec2_ssh_key = var.ssh_key_to_access_node
  }
  # Ensure that IAM Role permissions are created before and deleted after EKS Node Group handling.
#   # Otherwise, EKS will not be able to properly delete EC2 Instances and Elastic Network Interfaces.
#   # depends_on = [
#   #   aws_iam_role_policy_attachment.example-AmazonEKSWorkerNodePolicy,
#   #   aws_iam_role_policy_attachment.example-AmazonEKS_CNI_Policy,
#   #   aws_iam_role_policy_attachment.example-AmazonEC2ContainerRegistryReadOnly,
#   # ]
#   tags = {
#     Name = "${var.PROJECT_NAME}-eks-public-nodegroup"
# #     env  = "${var.ENV}"
#     # Cluster Autoscaler TAGS - CA needs below tags to identify and ADD the instances in ASG
#     "k8s.io/cluster-autoscaler/enabled"                         = "any-value" // ANY VALUE IF OKAY
#     "k8s.io/cluster-autoscaler/${aws_eks_cluster.cluster.name}" = "any-value" // ANY VALUE IF OKAY
#   }
}
*/

######## Private nodegroup
resource "aws_eks_node_group" "private-nodegroup" {
  cluster_name = aws_eks_cluster.cluster.name
  node_group_name = "${var.project-name}-eks-private-nodegroup"
  node_role_arn = var.eks_main_nodegroup_role_arn
  subnet_ids = [ var.pri_sub_3_a_id, var.pri_sub_4_b_id ]
  version = var.cluster_version
  ami_type = var.AMI_type
  instance_types = var.instance_type
  capacity_type = var.node_capacity_type
  disk_size = var.node_disk_size
  scaling_config {
    desired_size = var.desired_node
    max_size = var.max_node
    min_size = var.min_node
  }
  update_config {
    max_unavailable = 1
  }
  # üìù Uncomment below lines if you want to attach secret key(.pem file) for SSH into ec2 machine
  # remote_access {
  #   ec2_ssh_key = var.SSH_KEY_TO_ACCESS_NODE
  # }

  # Ensure that IAM Role permissions are created before and deleted after EKS Node Group handling.
  # Otherwise, EKS will not be able to properly delete EC2 Instances and Elastic Network Interfaces.
  # depends_on = [
  #   aws_iam_role_policy_attachment.example-AmazonEKSWorkerNodePolicy,
  #   aws_iam_role_policy_attachment.example-AmazonEKS_CNI_Policy,
  #   aws_iam_role_policy_attachment.example-AmazonEC2ContainerRegistryReadOnly,
  # ]
  tags = {
    Name = "${var.project_name}-eks-private-nodegroup"
    env = "${var.ENV}"
    # Cluster Autoscaler TAGS - CA needs below tags to identify and ADD the instances in ASG
    "k8s.io/cluster-autoscaler/enabled"                         = "any-value" // ANY VALUE IS OKAY
    "k8s.io/cluster-autoscaler/${aws_eks_cluster.cluster.name}" = "any-value" // ANY VALUE IS OKAY
  }
  
}
data "aws_partition" "current" {}

data "tls_certificate" "get_thumbprint" {
  # we are using this provider to get the root ca thumbprint DYNAMICALLY
  # üëâ  https://github.com/hashicorp/terraform-provider-tls/issues/52
  url = aws_eks_cluster.cluster.identity[0].oidc[0].issuer
}
resource "aws_iam_openid_connect_provider" "eks-oidc" {
  url = aws_eks_cluster.cluster.identity[0].oidc[0].issuer

  client_id_list = ["sts.${data.aws_partition.current.dns_suffix}"]

  # thumbprint_list = [var.EKS_OIDC_ROOT_CA_THUMBPRINT]
  thumbprint_list = [data.tls_certificate.get_thumbprint.certificates[0].sha1_fingerprint]
  tags = {
    Name = "${var.project_name}-eks-oidc-connect-provider "
    env  = "${var.ENV}"
  }
  
}

